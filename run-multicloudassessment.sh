#!/bin/bash
set -euo pipefail

###### Version 3.5 - Detailed Logging for ECS
SESSION_ID=$(uuidgen)
START_TIME=$(date +%s)
LOG_PREFIX="[RUNNER:$SESSION_ID]"

log() {
  local level="$1"; shift
  local msg="$*"
  local timestamp
  timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
  echo "${LOG_PREFIX} ${timestamp} [${level}] ${msg}"
}

log INFO "üß≠ Iniciando execu√ß√£o do Multicloud Assessment Runner"

# ==============================
# VARI√ÅVEIS B√ÅSICAS
# ==============================
TIMESTAMP=$(date +"%Y%m%d-%H%M%S")
OUTPUT_DIR="/tmp/output-${TIMESTAMP}"
mkdir -p "$OUTPUT_DIR"
log INFO "Diret√≥rio de sa√≠da: ${OUTPUT_DIR}"

# ==============================
# FUN√á√ïES AUXILIARES
# ==============================
get_param() {
  aws ssm get-parameter --name "$1" --with-decryption --query "Parameter.Value" --output text 2>/dev/null || echo ""
}

fetch_credentials() {
  local path="/clients/${CLIENT_NAME}/${CLOUD_PROVIDER}/${ACCOUNT_ID}/credentials/access"
  log INFO "Buscando credenciais em ${path}"
  get_param "$path"
}

authenticate() {
  local creds start_auth end_auth duration
  start_auth=$(date +%s)
  creds=$(fetch_credentials)

  if [ -z "$creds" ]; then
    log WARN "Nenhum par√¢metro encontrado no SSM. Solicitando credenciais manualmente..."
  fi

  case "$CLOUD_PROVIDER" in
    aws)
      if [ -z "$creds" ]; then
        read -rp "AWS_ACCESS_KEY_ID: " AWS_ACCESS_KEY_ID
        read -rp "AWS_SECRET_ACCESS_KEY: " AWS_SECRET_ACCESS_KEY
        read -rp "AWS_SESSION_TOKEN (opcional): " AWS_SESSION_TOKEN || true
      else
        if echo "$creds" | base64 --decode &>/dev/null; then
          echo "$creds" | base64 --decode > /tmp/aws_creds.json
        else
          echo "$creds" > /tmp/aws_creds.json
        fi
        export AWS_ACCESS_KEY_ID=$(jq -r '.AWS_ACCESS_KEY_ID' /tmp/aws_creds.json)
        export AWS_SECRET_ACCESS_KEY=$(jq -r '.AWS_SECRET_ACCESS_KEY' /tmp/aws_creds.json)
        export AWS_SESSION_TOKEN=$(jq -r '.AWS_SESSION_TOKEN // empty' /tmp/aws_creds.json)
      fi
      aws sts get-caller-identity >/dev/null 2>&1 && log INFO "‚úÖ Autentica√ß√£o AWS bem-sucedida" || log ERROR "‚ùå Falha na autentica√ß√£o AWS"
      ;;
    azure)
      if [ -z "$creds" ]; then
        read -rp "AZURE_TENANT_ID: " AZURE_TENANT_ID
        read -rp "AZURE_CLIENT_ID: " AZURE_CLIENT_ID
        read -rp "AZURE_CLIENT_SECRET: " AZURE_CLIENT_SECRET
      else
        if echo "$creds" | base64 --decode &>/dev/null; then
          echo "$creds" | base64 --decode > /etc/prowler/credentials/azure.env
        else
          echo "$creds" > /etc/prowler/credentials/azure.env
        fi
        source /etc/prowler/credentials/azure.env
      fi
      az login --service-principal -u "$AZURE_CLIENT_ID" -p "$AZURE_CLIENT_SECRET" --tenant "$AZURE_TENANT_ID" >/dev/null 2>&1 \
        && log INFO "‚úÖ Autentica√ß√£o Azure bem-sucedida" \
        || log ERROR "‚ùå Falha na autentica√ß√£o Azure"
      ;;
    gcp)
      mkdir -p /root/.config/gcloud
      if [ -z "$creds" ]; then
        read -rp "Caminho do arquivo JSON da Service Account: " SA_PATH
        cp "$SA_PATH" /root/.config/gcloud/application_default_credentials.json
      else
        if echo "$creds" | base64 --decode &>/dev/null; then
          echo "$creds" | base64 --decode > /root/.config/gcloud/application_default_credentials.json
        else
          echo "$creds" > /root/.config/gcloud/application_default_credentials.json
        fi
      fi
      gcloud auth activate-service-account --key-file=/root/.config/gcloud/application_default_credentials.json >/dev/null 2>&1 \
        && log INFO "‚úÖ Autentica√ß√£o GCP bem-sucedida" \
        || log ERROR "‚ùå Falha na autentica√ß√£o GCP"
      ;;
    *)
      log ERROR "‚ùå Provedor de nuvem desconhecido: ${CLOUD_PROVIDER}"
      exit 1
      ;;
  esac
  end_auth=$(date +%s)
  duration=$((end_auth - start_auth))
  log INFO "‚è±Ô∏è Dura√ß√£o da autentica√ß√£o (${CLOUD_PROVIDER}): ${duration}s"
}

# ==============================
# EXECU√á√ÉO DO SCAN
# ==============================
run_scan() {
  local output_file="${OUTPUT_DIR}/${CLOUD_PROVIDER}_${ACCOUNT_ID}_${TIMESTAMP}.json"
  log INFO "‚ñ∂Ô∏è Executando Prowler (${CLOUD_PROVIDER}) para ${ACCOUNT_ID}"
  local start_scan=$(date +%s)

  case "$CLOUD_PROVIDER" in
    aws)
      prowler aws \
        --provider aws \
        -M json-asff \
        --output-filename "$(basename "$output_file")" \
        --output-directory "$OUTPUT_DIR" \
        && log INFO "‚úÖ Scan AWS conclu√≠do" \
        || log WARN "‚ö†Ô∏è Falha parcial no scan AWS ($ACCOUNT_ID)"
      ;;
    azure)
      prowler azure \
        --subscription-ids "$ACCOUNT_ID" \
        -M json \
        --output-filename "$(basename "$output_file")" \
        --output-directory "$OUTPUT_DIR" \
        && log INFO "‚úÖ Scan Azure conclu√≠do" \
        || log WARN "‚ö†Ô∏è Falha parcial no scan Azure ($ACCOUNT_ID)"
      ;;
    gcp)
      prowler gcp \
        --project-ids "$ACCOUNT_ID" \
        -M json \
        --output-filename "$(basename "$output_file")" \
        --output-directory "$OUTPUT_DIR" \
        && log INFO "‚úÖ Scan GCP conclu√≠do" \
        || log WARN "‚ö†Ô∏è Falha parcial no scan GCP ($ACCOUNT_ID)"
      ;;
    *)
      log ERROR "‚ùå Provedor n√£o suportado: $CLOUD_PROVIDER"
      exit 1
      ;;
  esac

  if [ -f "$output_file" ]; then
    log INFO "üìÑ Relat√≥rio gerado: $output_file"
  else
    log ERROR "‚ö†Ô∏è Nenhum relat√≥rio gerado para ${CLOUD_PROVIDER}_${ACCOUNT_ID}"
  fi

  local end_scan=$(date +%s)
  log INFO "‚è±Ô∏è Dura√ß√£o do scan: $((end_scan - start_scan))s"
}

# ==============================
# UPLOAD PARA S3
# ==============================
upload_to_s3() {
  local s3_prefix="${CLIENT_NAME}/${CLOUD_PROVIDER}/${ACCOUNT_ID}/${TIMESTAMP}"
  local start_upload=$(date +%s)
  log INFO "üì§ Iniciando upload para s3://${S3_BUCKET}/${s3_prefix}/"

  aws s3 cp "$OUTPUT_DIR" "s3://${S3_BUCKET}/${s3_prefix}/" --recursive --region "$AWS_REGION" \
    && log INFO "‚úÖ Upload conclu√≠do" \
    || log ERROR "‚ùå Falha no upload para S3"

  local end_upload=$(date +%s)
  log INFO "‚è±Ô∏è Dura√ß√£o do upload: $((end_upload - start_upload))s"
}

# ==============================
# EXECU√á√ÉO PRINCIPAL
# ==============================
authenticate
run_scan
upload_to_s3

END_TIME=$(date +%s)
TOTAL_DURATION=$((END_TIME - START_TIME))
log INFO "üèÅ Execu√ß√£o finalizada com sucesso em ${TOTAL_DURATION}s"
log INFO "üìä Resultados dispon√≠veis em: s3://${S3_BUCKET}/${CLIENT_NAME}/${CLOUD_PROVIDER}/${ACCOUNT_ID}/${TIMESTAMP}/"
